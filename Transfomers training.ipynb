{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a62d887-01aa-4980-9220-3bf28c077504",
   "metadata": {},
   "source": [
    "### Домашнее задание Transformers Training (50 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d7452-febe-4d25-a9c1-005fcc26b35b",
   "metadata": {},
   "source": [
    "В этом домашнем задании требуется обучить несколько Transformer-based моделей в задаче машинного перевода. Для обучения можно воспользоваться текущим проектом, так и реализовать свой пайплайн обучения. Если будете использовать проект, теги **TODO** проекта отмечают, какие компоненты надо реализовать.\n",
    "В ноутбуке нужно только отобразить результаты обучения и выводы. Архитектура модели(количетсво слоев, размерность и тд) остается на ваш выбор.\n",
    "\n",
    "Ваш код обучения нужно выложить на ваш github, в строке ниже дать ссылку на него. В первую очередь будут оцениваться результаты в ноутбуке, код нужен для проверки адекватности результатов. \n",
    "\n",
    "Обучать модели до конца не нужно, только для демонстрации, что модель обучается и рабочая - снижение val_loss, рост bleu_score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a691e7b9-6538-42b1-96b8-838f2efab4af",
   "metadata": {},
   "source": [
    "#### Сcылка на ваш github с проектом(вставить свой) - https://github.com/StrongUzumaki/pytorch-machine-translation/tree/hw\n",
    "\n",
    "Ноутбук с результатами выкладывать на ваш **google диск** курса. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca1e62-b210-4426-9854-55b652417a4b",
   "metadata": {},
   "source": [
    "### Данные\n",
    "\n",
    "`\n",
    "wget https://www.manythings.org/anki/rus-eng.zip && unzip rus-eng.zip\n",
    "`\n",
    "\n",
    "Модели нужно обучить на задаче перевода с английского на русский. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "710e84de-f7a4-46da-a611-74c90a67e24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://www.manythings.org/anki/rus-eng.zip && unzip rus-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc772a6e-7d1a-4d8d-8024-0454a948835b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Обучение Seq2seq Transformer модель(25 баллов)\n",
    "\n",
    "Реализуйте Seq2seq Transformer. В качестве блока трансформера можно использовать https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html. В качестве токенизатора воспользуйтесь HuggingFace токенизатор для source/target языков - https://huggingface.co/docs/transformers/fast_tokenizers\n",
    "В качестве максимальной длинны возьмите предложения длинной **до 15 слов**, без каких либо префиксов. \n",
    "\n",
    "Не забудьте остальные элементы модели:\n",
    "* Мы можем использовать 1 трансформер как энкодер - декодером будет выступать линейный слой. \n",
    "* Обучите свой BPE токенизатор - https://huggingface.co/docs/transformers/fast_tokenizers\n",
    "* Матрицу эмбеддингов токенов\n",
    "* Матрицу позицонных эмбеддингов\n",
    "* Линейный слой проекции в target словарь\n",
    "* Функцию маскирования будущих состояний attention, так как модель авто-регрессионна\n",
    "* Learning rate schedualer\n",
    "\n",
    "\n",
    "В качестве результатов, приложите слудующие данные:\n",
    "1) Параметры обучения - learning rate, batch_size, epoch_num, размерность скрытого слоя, количетсво слоев\n",
    "2) Графики обучения - train loss, val loss, bleu score\n",
    "3) Примеры переводов вашей модели(10 штук) - source text, true target text, predicted target text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a7ce070-1e45-47ac-a07a-c36b6baa114e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/yarix/.local/lib/python3.11/site-packages (4.40.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/yarix/.local/lib/python3.11/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/yarix/.local/lib/python3.11/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/yarix/.local/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/yarix/.local/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/yarix/.local/lib/python3.11/site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/yarix/.local/lib/python3.11/site-packages (from transformers) (2024.5.10)\n",
      "Requirement already satisfied: filelock in /home/yarix/.local/lib/python3.11/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: requests in /home/yarix/.local/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/yarix/.local/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/yarix/.local/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/yarix/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/yarix/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yarix/.local/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yarix/.local/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yarix/.local/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yarix/.local/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "import ast\n",
    "from matplotlib import pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "!python3.11 -m pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cc74fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yarix/.local/lib/python3.11/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import train_seq2seq_transformer as tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8bc404d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading from file: 100%|██████████| 496059/496059 [00:06<00:00, 79310.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yarix/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/yarix/.local/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of tokenizers.Encoding",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, dm, train_dataloader, val_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mtst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/Lenovo/Desktop/MIPT/DZ_2_SEM/ML/pytorch-machine-translation/train_seq2seq_transformer.py:40\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     train_dataloader \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(train_dataloader)[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m     38\u001b[0m     dev_dataloader \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(train_dataloader)[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m---> 40\u001b[0m \u001b[43mtrainer_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, dm, train_dataloader, dev_dataloader\n",
      "File \u001b[0;32m/mnt/c/Users/Lenovo/Desktop/MIPT/DZ_2_SEM/ML/pytorch-machine-translation/src/models/trainer.py:18\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m     16\u001b[0m train_epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 18\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_epoch_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/mnt/c/Users/Lenovo/Desktop/MIPT/DZ_2_SEM/ML/pytorch-machine-translation/src/data/mt_dataset.py:15\u001b[0m, in \u001b[0;36mMTDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 15\u001b[0m     source_ids, target_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenized_source_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), \\\n\u001b[1;32m     16\u001b[0m                              torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenized_target_list[idx])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m source_ids, target_ids\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of tokenizers.Encoding"
     ]
    }
   ],
   "source": [
    "model, dm, train_dataloader, val_dataloader = tst.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245297ae-d62b-4227-9312-ccff5a1c13c9",
   "metadata": {},
   "source": [
    "### Fine-tune pretrained T5 (25 баллов)\n",
    "\n",
    "Реализуйте Seq2seq Pretrained T5. Воспользуйтесь https://huggingface.co/docs/transformers/model_doc/t5 предобученной моделью. В качестве максимальной длинны возьмите предложения длинной **до 15 слов**, без каких либо префиксов. Архитектура модели(количетсво слоев, размерность и тд) остается на ваш выбор.\n",
    "\n",
    "Не забудьте важные аспекты обучения модели:\n",
    "* Взять готовый t5 токенизатор\n",
    "* Resize matrix embedding - скорей всего ваша матрица эмбеддингов не будет включать эмбеддинги из вашего сета. Пример обновления матрицы эмбеддингов тут тут https://github.com/runnerup96/Transformers-Tuning/blob/main/t5_encoder_decoder.py\n",
    "* Learning rate schedualer/Adafactor with constant learning rate\n",
    "\n",
    "\n",
    "В качестве результатов, приложите слудующие данные:\n",
    "1) Параметры обучения - learning rate, batch_size, epoch_num, pretrained model name\n",
    "2) Графики обучения - train loss, val loss, bleu score\n",
    "3) Примеры переводов вашей модели(10 штук) - source text, true target text, predicted target text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5993bdc9-44f6-4af8-8e46-36d4e6824ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
